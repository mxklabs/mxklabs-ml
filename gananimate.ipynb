{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Round 0 predictions: 0.0001688399352133274 ----------\n",
      "Mean score is 0.0001688399352133274 -- training discriminator.\n",
      "Train on 1100 samples, validate on 275 samples\n",
      "1100/1100 [==============================] - 0s 299us/sample - loss: 0.7985 - binary_accuracy: 0.9091 - val_loss: 0.7043 - val_binary_accuracy: 0.9091\n",
      "---------- Round 1 predictions: 0.023544881492853165 ----------\n",
      "Mean score is 0.023544881492853165 -- training discriminator.\n",
      "Train on 1100 samples, validate on 275 samples\n",
      "1100/1100 [==============================] - 0s 51us/sample - loss: 0.4467 - binary_accuracy: 0.9091 - val_loss: 0.2749 - val_binary_accuracy: 0.9091\n",
      "---------- Round 2 predictions: 0.08639628440141678 ----------\n",
      "Mean score is 0.08639628440141678 -- training discriminator.\n",
      "Train on 1100 samples, validate on 275 samples\n",
      "1100/1100 [==============================] - 0s 50us/sample - loss: 0.2690 - binary_accuracy: 0.9091 - val_loss: 0.2542 - val_binary_accuracy: 0.9091\n",
      "---------- Round 3 predictions: 0.06154756620526314 ----------\n",
      "Mean score is 0.06154756620526314 -- training discriminator.\n",
      "Train on 1100 samples, validate on 275 samples\n",
      "1100/1100 [==============================] - 0s 52us/sample - loss: 0.2478 - binary_accuracy: 0.9091 - val_loss: 0.2420 - val_binary_accuracy: 0.9091\n",
      "---------- Round 4 predictions: 0.04071435704827309 ----------\n",
      "Mean score is 0.04071435704827309 -- training discriminator.\n",
      "Train on 1100 samples, validate on 275 samples\n",
      "1100/1100 [==============================] - 0s 52us/sample - loss: 0.2410 - binary_accuracy: 0.9091 - val_loss: 0.2464 - val_binary_accuracy: 0.9091\n",
      "---------- Round 5 predictions: 0.03850851207971573 ----------\n",
      "Mean score is 0.03850851207971573 -- training discriminator.\n",
      "Train on 1100 samples, validate on 275 samples\n",
      "1100/1100 [==============================] - 0s 51us/sample - loss: 0.2409 - binary_accuracy: 0.9091 - val_loss: 0.2410 - val_binary_accuracy: 0.9091\n",
      "---------- Round 6 predictions: 0.03890535980463028 ----------\n",
      "Mean score is 0.03890535980463028 -- training discriminator.\n",
      "Train on 1100 samples, validate on 275 samples\n",
      "1100/1100 [==============================] - 0s 60us/sample - loss: 0.2334 - binary_accuracy: 0.9091 - val_loss: 0.2276 - val_binary_accuracy: 0.9091\n",
      "---------- Round 7 predictions: 0.005735938902944326 ----------\n",
      "Mean score is 0.005735938902944326 -- training discriminator.\n",
      "Train on 1100 samples, validate on 275 samples\n",
      "1100/1100 [==============================] - 0s 58us/sample - loss: 0.2343 - binary_accuracy: 0.9091 - val_loss: 0.2281 - val_binary_accuracy: 0.9091\n",
      "---------- Round 8 predictions: 0.01099655032157898 ----------\n",
      "Mean score is 0.01099655032157898 -- training discriminator.\n",
      "Train on 1100 samples, validate on 275 samples\n",
      "1100/1100 [==============================] - 0s 51us/sample - loss: 0.2266 - binary_accuracy: 0.9091 - val_loss: 0.2266 - val_binary_accuracy: 0.9091\n",
      "---------- Round 9 predictions: 0.02520461566746235 ----------\n",
      "Mean score is 0.02520461566746235 -- training discriminator.\n",
      "Train on 1100 samples, validate on 275 samples\n",
      "1100/1100 [==============================] - 0s 60us/sample - loss: 0.2298 - binary_accuracy: 0.9091 - val_loss: 0.2147 - val_binary_accuracy: 0.9091\n",
      "---------- Round 10 predictions: 0.0026894500479102135 ----------\n",
      "Mean score is 0.0026894500479102135 -- training discriminator.\n",
      "Train on 1100 samples, validate on 275 samples\n",
      "1100/1100 [==============================] - 0s 50us/sample - loss: 0.2281 - binary_accuracy: 0.9091 - val_loss: 0.2274 - val_binary_accuracy: 0.9091\n",
      "---------- Round 11 predictions: -0.023539608344435692 ----------\n",
      "Mean score is -0.023539608344435692 -- training discriminator.\n",
      "Train on 1100 samples, validate on 275 samples\n",
      "1100/1100 [==============================] - 0s 55us/sample - loss: 0.2255 - binary_accuracy: 0.9091 - val_loss: 0.2294 - val_binary_accuracy: 0.9091\n",
      "---------- Round 12 predictions: -0.012269428931176662 ----------\n",
      "Mean score is -0.012269428931176662 -- training discriminator.\n",
      "Train on 1100 samples, validate on 275 samples\n",
      "1100/1100 [==============================] - 0s 60us/sample - loss: 0.2203 - binary_accuracy: 0.9091 - val_loss: 0.2174 - val_binary_accuracy: 0.9091\n",
      "---------- Round 13 predictions: -0.005948545411229134 ----------\n",
      "Mean score is -0.005948545411229134 -- training discriminator.\n",
      "Train on 1100 samples, validate on 275 samples\n",
      "1100/1100 [==============================] - 0s 51us/sample - loss: 0.2231 - binary_accuracy: 0.9091 - val_loss: 0.2187 - val_binary_accuracy: 0.9091\n",
      "---------- Round 14 predictions: -0.01639304682612419 ----------\n",
      "Mean score is -0.01639304682612419 -- training discriminator.\n",
      "Train on 1100 samples, validate on 275 samples\n",
      "1100/1100 [==============================] - 0s 58us/sample - loss: 0.2189 - binary_accuracy: 0.9091 - val_loss: 0.2161 - val_binary_accuracy: 0.9091\n",
      "---------- Round 15 predictions: -0.03205885738134384 ----------\n",
      "Mean score is -0.03205885738134384 -- training discriminator.\n",
      "Train on 1100 samples, validate on 275 samples\n",
      "1100/1100 [==============================] - 0s 53us/sample - loss: 0.2263 - binary_accuracy: 0.9091 - val_loss: 0.2372 - val_binary_accuracy: 0.9091\n",
      "---------- Round 16 predictions: 0.009300552308559418 ----------\n",
      "Mean score is 0.009300552308559418 -- training discriminator.\n",
      "Train on 1100 samples, validate on 275 samples\n",
      "1100/1100 [==============================] - 0s 51us/sample - loss: 0.2210 - binary_accuracy: 0.9091 - val_loss: 0.2119 - val_binary_accuracy: 0.9091\n",
      "---------- Round 17 predictions: -0.049667954444885254 ----------\n",
      "Mean score is -0.049667954444885254 -- training discriminator.\n",
      "Train on 1100 samples, validate on 275 samples\n",
      "1100/1100 [==============================] - 0s 57us/sample - loss: 0.3146 - binary_accuracy: 0.8909 - val_loss: 0.2938 - val_binary_accuracy: 0.9091\n",
      "---------- Round 18 predictions: 0.11718766391277313 ----------\n",
      "Mean score is 0.11718766391277313 -- training discriminator.\n",
      "Train on 1100 samples, validate on 275 samples\n",
      "1100/1100 [==============================] - 0s 52us/sample - loss: 0.2509 - binary_accuracy: 0.9082 - val_loss: 0.2351 - val_binary_accuracy: 0.9091\n",
      "---------- Round 19 predictions: 0.031368792057037354 ----------\n",
      "Mean score is 0.031368792057037354 -- training discriminator.\n",
      "Train on 1100 samples, validate on 275 samples\n",
      "1100/1100 [==============================] - 0s 54us/sample - loss: 0.2246 - binary_accuracy: 0.9091 - val_loss: 0.2178 - val_binary_accuracy: 0.9091\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import itertools\n",
    "import functools\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.collections import PathCollection\n",
    "\n",
    "from gan import Gan, Data\n",
    "import pltanimation as plta\n",
    "\n",
    "NUM_SAMPLES=100\n",
    "\n",
    "dm = 20   # discriminator granularity\n",
    "ds = 0.025 # discriminator square size\n",
    "tics = [(t+0.5)/dm for t in range(-dm, dm, 1)]\n",
    "tics_xy = np.array(list(itertools.product(tics, tics)))\n",
    "\n",
    "def animation_init(data, fig):    \n",
    "  ax = fig.add_subplot(1,1,1)\n",
    "  ax.set_title(\"Empty plot\")\n",
    "  #plot1 = ax.plot([], [])[0]\n",
    "  real = ax.scatter(data.real_x[:,0], data.real_x[:,1], label='real', s=10)\n",
    "  generator_out = ax.scatter([], [], label='generator', s=5)\n",
    "\n",
    "  box = ax.add_patch(Rectangle((0, 0), 0.5, 0.5, \n",
    "        linewidth=0, edgecolor='b', facecolor=(0.6,1.0,0.6), zorder=-1))\n",
    "    \n",
    "  boxes = []\n",
    "  \n",
    "  for x, y in tics_xy:\n",
    "    boxes.append(ax.add_patch(Rectangle((x-ds, y-ds), 2*ds, 2*ds, \n",
    "      linewidth=0, edgecolor='b', facecolor=(0.6,1.0,0.6), zorder=-1)))\n",
    "    \n",
    "  \"\"\"\n",
    "    \n",
    "    tics_xy = np.array(list(itertools.product(tics, tics)))\n",
    "    tics_pred = discriminator.predict(tics_xy)\n",
    "    dis_items = np.concatenate((tics_xy, tics_pred), axis=-1)\n",
    "    dis_items = dis_items[(dis_items[:,2] >= 0.5)]\n",
    "    for x, y, pred in dis_items:\n",
    "      plt.gca().add_patch(plt.Rectangle((x-ds, y-ds), 2*ds, 2*ds, \n",
    "        linewidth=0, edgecolor='b',facecolor=(0.6,1.0,0.6),zorder=-1))\n",
    "  \"\"\"\n",
    "    \n",
    "  ax.legend(loc='upper left')\n",
    "  ax.grid(True)\n",
    "  ax.set_xlim((-1,1))\n",
    "  ax.set_ylim((-1,1))\n",
    "  return [generator_out] + boxes\n",
    "\n",
    "def animation_callback(ani, gan_model, generator, discriminator, generator_output, discriminator_output, score):\n",
    "\n",
    "    box_updates = []\n",
    "    for x, y in tics_xy:\n",
    "      out = discriminator.predict([[x, y]])[0]\n",
    "      if (out >= 0.5):\n",
    "          box_updates.append((Rectangle.set_facecolor, (0.6,1.0,0.6)))\n",
    "      else:\n",
    "          box_updates.append((Rectangle.set_facecolor, (1,1,1))) \n",
    "\n",
    "    ani.add_frame([\n",
    "        (PathCollection.set_offsets, generator_output)        \n",
    "    ] + box_updates)\n",
    "\n",
    "def gen_data(num_samples):\n",
    "  noise = np.array([random.normalvariate(0, 0.4) for _ in range(num_samples)])\n",
    "  x1 = np.array([random.random() * 2 - 1 for _ in range(num_samples)])\n",
    "  x2 = np.array((np.power(x1*2, 3) + np.sin(x1*12 - 2)*0.7 + noise)/7)\n",
    "  y = np.full(shape=(num_samples, 1), fill_value=1)\n",
    "  x1 = x1.reshape(-1,1)\n",
    "  x2 = x2.reshape(-1,1)    \n",
    "  return np.concatenate((x1, x2), axis=-1), y\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "  real_x, real_y = gen_data(NUM_SAMPLES)\n",
    "  real_x_val, real_y_val = gen_data(NUM_SAMPLES//4)\n",
    "  data = Data(real_x=real_x, real_y=real_y, real_x_val=real_x_val, real_y_val=real_y_val)\n",
    "  \n",
    "  ani = plta.PltAnimation(functools.partial(animation_init, data), figsize=(10, 10)) \n",
    "    \n",
    "  latent_shape = (5,)\n",
    "  data_shape = (2,) # data shape\n",
    "  num_samples = 40\n",
    "\n",
    "  # Get real words for training.\n",
    "  latent_space = tf.keras.layers.Input(shape=latent_shape)\n",
    "\n",
    "  # Using Leaky RELU because of tip https://machinelearningmastery.com/how-to-train-stable-generative-adversarial-networks/.\n",
    "\n",
    "  # Discriminator input is (n, 21, 27) output is (n, 1)\n",
    "  discriminator = tf.keras.models.Sequential(name=\"discriminator\")\n",
    "  discriminator.add(tf.keras.layers.Dense(name='dhidden1', units=50, use_bias=True,\n",
    "                                          activation=tf.keras.layers.LeakyReLU(alpha=0.3), input_shape=data_shape))\n",
    "  discriminator.add(tf.keras.layers.Dense(name='dhidden2', units=50, use_bias=True,\n",
    "                                          activation=tf.keras.layers.LeakyReLU(alpha=0.3)))\n",
    "  discriminator.add(tf.keras.layers.Dense(name='dhidden3', units=15, use_bias=True,\n",
    "                                          activation=tf.keras.layers.LeakyReLU(alpha=0.3)))\n",
    "  discriminator.add(tf.keras.layers.Dense(name='doutput', units=1, use_bias=True, activation='tanh'))\n",
    "\n",
    "  #discriminator.add(tf.keras.layers.Dense(name='dhidden1', units=50, use_bias=True, activation='tanh', input_shape=data_shape))\n",
    "  #discriminator.add(tf.keras.layers.Dense(name='dhidden2', units=50, use_bias=True, activation='tanh'))\n",
    "  #discriminator.add(tf.keras.layers.Dense(name='dhidden3', units=15, use_bias=True, activation='tanh'))\n",
    "  #discriminator.add(tf.keras.layers.Dense(name='doutput', units=1, use_bias=True, activation='linear'))\n",
    "  discriminator.compile(optimizer=tf.keras.optimizers.Adam(0.0005), loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "\n",
    "  # Generator.\n",
    "  discriminator.trainable = False\n",
    "\n",
    "  generator = tf.keras.models.Sequential(name=\"generator\")\n",
    "  generator.add(tf.keras.layers.Dense(name='ghidden1', units=15, use_bias=True, activation='tanh', input_shape=latent_shape))\n",
    "  generator.add(tf.keras.layers.Dense(name='ghidden2', units=15, use_bias=True, activation='tanh'))\n",
    "  generator.add(tf.keras.layers.Dense(name='gout', units=2, use_bias=True, activation='linear'))\n",
    "\n",
    "  gan_model = tf.keras.models.Model(latent_space, discriminator(generator(latent_space)))\n",
    "  gan_model.compile(optimizer=tf.keras.optimizers.Adam(0.0005), loss='mse', metrics=['binary_accuracy'])\n",
    "\n",
    "  gan = Gan(gan_model, data)\n",
    "  gan.train(iterations=20, epochs_per_round=1, num_samples=1000, #, train_discriminator_only=True, \n",
    "            verbose=1, callback=functools.partial(animation_callback, ani))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani.as_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
